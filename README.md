# NLP Word Embedding using Word2Vec

This project demonstrates how to generate word embeddings using the Word2Vec technique in Natural Language Processing (NLP).

## ğŸ“Œ Objective
To convert textual data into meaningful numerical vectors (embeddings) that capture semantic relationships between words.

## ğŸ§  Concepts Used
- Tokenization
- Text preprocessing
- Word2Vec model
- Vector representation of words
- Similarity between words

## ğŸ“‚ Project Structure
- `NLP_word_embedding.ipynb` â€” Complete implementation notebook

## ğŸ› ï¸ Libraries Used
- pandas
- numpy
- gensim
- nltk
- matplotlib

## ğŸš€ How to Run
1. Install required libraries from `requirements.txt`
2. Open the notebook
3. Run all cells sequentially

## ğŸ“ˆ Outcome
The model learns vector representations where semantically similar words are closer in vector space.

---

â­ This project is part of my Machine Learning & NLP learning journey.